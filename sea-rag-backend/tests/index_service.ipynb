{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eaa9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 代码块 1: 导入和初始化 =====\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# 加载环境变量 (自动寻找项目根目录的 .env)\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# 设置数据目录\n",
    "DATA_ROOT = Path(\"data_debug\")\n",
    "\n",
    "print(\"✅ 模块导入和初始化完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d916560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 目录管理函数\n",
    "def workdir(file_id: str) -> Path:\n",
    "    p = DATA_ROOT / file_id\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def markdown_path(file_id: str) -> Path:\n",
    "    return workdir(file_id) / \"output.md\"\n",
    "\n",
    "def index_dir(file_id: str) -> Path:\n",
    "    p = workdir(file_id) / \"index_faiss\"\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "print(\"Path helpers defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55bee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: 生成测试 Markdown 文件\n",
    "FILE_ID = \"test_doc_001\"\n",
    "\n",
    "dummy_md_content = \"\"\"\n",
    "# 项目介绍\n",
    "这是一个用于测试 RAG 流程的文档。这里是项目介绍的正文部分。\n",
    "\n",
    "## 技术栈\n",
    "我们使用了 Python, LangChain 和 FAISS。\n",
    "- Python: 编程语言\n",
    "- FAISS: 向量库\n",
    "\n",
    "# 部署指南\n",
    "## 环境准备\n",
    "请确保安装了 Python 3.10 以上版本。\n",
    "\n",
    "## 启动服务\n",
    "运行 `python main.py` 启动服务。\n",
    "\"\"\"\n",
    "\n",
    "# 写入文件\n",
    "md_file = markdown_path(FILE_ID)\n",
    "md_file.write_text(dummy_md_content, encoding=\"utf-8\")\n",
    "print(f\"Created dummy markdown at: {md_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: 测试 Markdown 切分\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "def split_markdown(md_text: str) -> List[Document]:\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        # (\"###\", \"Header 3\") # 需要更细可以加\n",
    "    ]\n",
    "    splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    docs = splitter.split_text(md_text)\n",
    "    \n",
    "    # 清洗逻辑\n",
    "    cleaned: List[Document] = []\n",
    "    for d in docs:\n",
    "        txt = (d.page_content or \"\").strip()\n",
    "        if not txt:\n",
    "            continue\n",
    "        if len(txt) > 8000:\n",
    "            txt = txt[:8000]\n",
    "        cleaned.append(Document(page_content=txt, metadata=d.metadata))\n",
    "    return cleaned\n",
    "\n",
    "# 读取刚才生成的 MD 并测试\n",
    "md_text = markdown_path(FILE_ID).read_text(encoding=\"utf-8\")\n",
    "docs = split_markdown(md_text)\n",
    "\n",
    "print(f\"Split into {len(docs)} chunks.\\n\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"--- Chunk {i+1} ---\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f945db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: 加载 Embedding 模型 (根据rag_agent.py修改)\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain_openai import OpenAIEmbeddings # 原OpenAI实现\n",
    "# from langchain_community.embeddings import FakeEmbeddings # 仅用于无 Key 调试\n",
    "\n",
    "def load_embeddings():\n",
    "    # --- 调试选项：使用本地模型替代OpenAI ---\n",
    "    # 优先从环境变量读取模型路径/名称\n",
    "    model_name = os.getenv(\"EMBEDDING_MODEL_NAME\", \"BAAI/bge-small-zh-v1.5\")\n",
    "    print(f\"正在加载 Embedding 模型: {model_name} ...\")\n",
    "    try:\n",
    "        # 使用与rag_agent.py相同的本地模型\n",
    "        embed = HuggingFaceBgeEmbeddings(\n",
    "            model_name=os.getenv(\"EMBEDDING_MODEL_NAME\", \"BAAI/bge-small-zh-v1.5\"),\n",
    "            model_kwargs={'device': 'cpu'},  # 如果有GPU可以改为'cuda'\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        return embed\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 本地模型加载失败: {e}\")\n",
    "        \n",
    "        # 回退到OpenAI（如果有API Key）\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\") or os.getenv(\"OPENAI_EMBEDDING_API_KEY\")\n",
    "        base_url = os.getenv(\"OPENAI_BASE_URL\") or os.getenv(\"OPENAI_EMBEDDING_BASE_URL\")\n",
    "        \n",
    "        if api_key:\n",
    "            print(\"回退到OpenAI Embeddings...\")\n",
    "            kwargs = {\"api_key\": api_key}\n",
    "            if base_url:\n",
    "                kwargs[\"base_url\"] = base_url\n",
    "            return OpenAIEmbeddings(model=\"text-embedding-3-large\", **kwargs)\n",
    "        else:\n",
    "            # 最后回退到假嵌入（仅用于测试）\n",
    "            print(\"⚠️ Warning: No API Key found. Using FakeEmbeddings (dimension=512).\")\n",
    "            from langchain_community.embeddings import FakeEmbeddings \n",
    "            return FakeEmbeddings(size=512) # 注意：bge-small-zh-v1.5的维度是512\n",
    "\n",
    "# 测试加载\n",
    "emb_model = load_embeddings()\n",
    "# 试着 embed 一个词测试连通性\n",
    "try:\n",
    "    vec = emb_model.embed_query(\"test\")\n",
    "    print(f\"Embedding check passed. Vector dimension: {len(vec)}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Embedding connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b9a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: 构建并保存索引\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def build_faiss_index(file_id: str) -> Dict[str, Any]:\n",
    "    md_file = markdown_path(file_id)\n",
    "    if not md_file.exists():\n",
    "        return {\"ok\": False, \"error\": \"MARKDOWN_NOT_FOUND\"}\n",
    "    \n",
    "    md_text = md_file.read_text(encoding=\"utf-8\")\n",
    "    docs = split_markdown(md_text)\n",
    "    \n",
    "    if not docs:\n",
    "        return {\"ok\": False, \"error\": \"EMPTY_MD\"}\n",
    "\n",
    "    print(f\"Vectorizing {len(docs)} documents...\")\n",
    "    embeddings = load_embeddings()\n",
    "    try:\n",
    "        vs = FAISS.from_documents(docs, embedding=embeddings)\n",
    "        save_path = index_dir(file_id)\n",
    "        vs.save_local(str(save_path))\n",
    "        print(f\"Index saved to {save_path}\")\n",
    "        return {\"ok\": True, \"chunks\": len(docs)}\n",
    "    except Exception as e:\n",
    "        print(f\"Error building index: {e}\")\n",
    "        return {\"ok\": False, \"error\": str(e)}\n",
    "\n",
    "# 运行构建\n",
    "build_result = build_faiss_index(FILE_ID)\n",
    "print(\"Build Result:\", build_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b29f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: 搜索测试\n",
    "def search_faiss(file_id: str, query: str, k: int = 5) -> Dict[str, Any]:\n",
    "    idx = index_dir(file_id)\n",
    "    if not (idx / \"index.faiss\").exists():\n",
    "        return {\"ok\": False, \"error\": \"INDEX_NOT_FOUND\"}\n",
    "\n",
    "    print(f\"Loading index from {idx} ...\")\n",
    "    embeddings = load_embeddings()\n",
    "    \n",
    "    # allow_dangerous_deserialization=True 是必须的，因为 FAISS 使用 pickle\n",
    "    vs = FAISS.load_local(str(idx), embeddings, allow_dangerous_deserialization=True)\n",
    "    \n",
    "    print(f\"Searching for: '{query}'\")\n",
    "    hits = vs.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    results = []\n",
    "    for doc, score in hits:\n",
    "        results.append({\n",
    "            \"text\": doc.page_content,\n",
    "            \"score\": float(score), # FAISS L2 距离，越小越相似\n",
    "            \"metadata\": doc.metadata,\n",
    "        })\n",
    "    return {\"ok\": True, \"results\": results}\n",
    "\n",
    "# 运行搜索\n",
    "# 尝试搜索 \"环境\" 相关的词\n",
    "search_result = search_faiss(FILE_ID, \"如何准备环境？\", k=2)\n",
    "\n",
    "if search_result[\"ok\"]:\n",
    "    for i, item in enumerate(search_result[\"results\"]):\n",
    "        print(f\"\\nResult {i+1} (Score: {item['score']:.4f}):\")\n",
    "        print(f\"Text: {item['text']}\")\n",
    "        print(f\"Meta: {item['metadata']}\")\n",
    "else:\n",
    "    print(\"Search failed:\", search_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
