{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dae78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: 导入与环境配置\n",
    "import os\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# 修复：使用新的导入路径\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# 1. 加载环境变量 (自动寻找项目根目录的 .env)\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# 2. 定义调试用的 File ID 和目录\n",
    "DEBUG_FILE_ID = \"debug_rag_001\"\n",
    "DATA_ROOT = Path(\"data\")\n",
    "INDEX_PATH = DATA_ROOT / DEBUG_FILE_ID / \"index_faiss\"\n",
    "\n",
    "# 3. 创建 Mock 索引数据的函数\n",
    "def create_mock_index():\n",
    "    if INDEX_PATH.exists():\n",
    "        print(f\"Index already exists at {INDEX_PATH}, skipping creation.\")\n",
    "        return\n",
    "\n",
    "    print(\"Creating mock FAISS index for debugging...\")\n",
    "    # 模拟一些课程内容\n",
    "    texts = [\n",
    "        \"本课程主要讲解 Python 高级编程。\",\n",
    "        \"在 RAG 系统中，Retrieval 指的是检索环节，Generation 指的是生成环节。\",\n",
    "        \"DeepSeek 是一个强大的开源大模型，尤其在编码和数学领域表现优异。\",\n",
    "        \"FAISS 是 Facebook 开源的向量检索库，用于高效的相似度搜索。\",\n",
    "        \"要启动本项目，请运行 `python server.py`，默认端口为 8000。\"\n",
    "    ]\n",
    "    docs = [Document(page_content=t, metadata={\"page\": i+1}) for i, t in enumerate(texts)]\n",
    "    \n",
    "    # 初始化 Embedding\n",
    "    emb = HuggingFaceBgeEmbeddings(\n",
    "            model_name=os.getenv(\"EMBEDDING_MODEL_NAME\", \"BAAI/bge-small-zh-v1.5\"),\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    vs = FAISS.from_documents(docs, embedding=emb)\n",
    "    INDEX_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    vs.save_local(str(INDEX_PATH))\n",
    "    print(\"Mock index created successfully.\")\n",
    "\n",
    "# 执行创建\n",
    "try:\n",
    "    create_mock_index()\n",
    "except Exception as e:\n",
    "    print(f\"Error creating mock index: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa649d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 核心逻辑定义\n",
    "from typing import List, Dict, Any, Tuple, AsyncGenerator\n",
    "from collections import defaultdict\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# --- 1. 状态管理 ---\n",
    "_sessions: dict[str, list[dict]] = defaultdict(list)\n",
    "\n",
    "def get_history(session_id: str) -> list[dict]:\n",
    "    return _sessions.get(session_id, [])\n",
    "\n",
    "def append_history(session_id: str, role: str, content: str) -> None:\n",
    "    _sessions[session_id].append({\"role\": role, \"content\": content})\n",
    "\n",
    "def clear_history(session_id: str) -> None:\n",
    "    _sessions.pop(session_id, None)\n",
    "\n",
    "# --- 2. 配置常量 (已同步 rag_agent.py) ---\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-V3.2\" \n",
    "SILICON_BASE_URL = \"https://api.siliconflow.cn/v1\"\n",
    "TEMPERATURE = 0\n",
    "K = 3\n",
    "SCORE_TAU_TOP1 = 0.45\n",
    "SCORE_TAU_MEAN3 = 0.60\n",
    "\n",
    "# --- 3. Prompt ---\n",
    "SYSTEM_INSTRUCTION = \"你是九天老师团队开发的多模态 PDF 检索 RAG 聊天机器人...\"\n",
    "GRADE_PROMPT = \"\\nTask: Assess the relevance of a retrieved document to a user question.\\n\\nRetrieved document:\\n{context}\\n\\nUser question: {question}\\n\\nReturn 'yes' if relevant, otherwise 'no'.\"\n",
    "ANSWER_WITH_CONTEXT = \"请使用提供的上下文回答用户的问题...\\n{context}...\\nQuestion: {question}\"\n",
    "ANSWER_NO_CONTEXT = \"当前未找到与课程资料直接相关的片段...\"\n",
    "\n",
    "# --- 4. 辅助函数 ---\n",
    "def _get_llm():\n",
    "    return init_chat_model(\n",
    "        model=MODEL_NAME,\n",
    "        model_provider=\"openai\", \n",
    "        openai_api_base=SILICON_BASE_URL,\n",
    "        temperature=TEMPERATURE\n",
    "    )\n",
    "\n",
    "def _get_grader():\n",
    "    return init_chat_model(\n",
    "        model=MODEL_NAME,\n",
    "        model_provider=\"openai\",\n",
    "        openai_api_base=SILICON_BASE_URL,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "def _get_embeddings():\n",
    "    return HuggingFaceBgeEmbeddings(\n",
    "            model_name=os.getenv(\"EMBEDDING_MODEL_NAME\", \"BAAI/bge-small-zh-v1.5\"),\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "\n",
    "def _vs_dir(file_id: str) -> str:\n",
    "    return str(DATA_ROOT / file_id / \"index_faiss\")\n",
    "\n",
    "def _load_vs(file_id: str) -> FAISS:\n",
    "    vs_path = _vs_dir(file_id)\n",
    "    if not os.path.exists(vs_path):\n",
    "        raise FileNotFoundError(f\"FAISS index not found at {vs_path}\")\n",
    "    return FAISS.load_local(vs_path, _get_embeddings(), allow_dangerous_deserialization=True)\n",
    "\n",
    "def _score_ok(scores: List[float]) -> bool:\n",
    "    if not scores: return False\n",
    "    top1 = scores[0]\n",
    "    mean3 = sum(scores[:3]) / min(3, len(scores))\n",
    "    print(f\"[Debug] Scores: {scores}, Top1: {top1}, Mean3: {mean3}\")\n",
    "    return (top1 <= SCORE_TAU_TOP1) or (mean3 <= SCORE_TAU_MEAN3)\n",
    "\n",
    "print(\"Service functions defined with SiliconFlow config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: 调试检索 (retrieve)\n",
    "async def retrieve(question: str, file_id: str) -> tuple[list[dict], str]:\n",
    "    print(f\"--- Retrieving for: {question} ---\")\n",
    "    vs = _load_vs(file_id)\n",
    "    hits = vs.similarity_search_with_score(question, k=K)\n",
    "    \n",
    "    citations = []\n",
    "    ctx_snippets = []\n",
    "    scores = []\n",
    "    \n",
    "    for i, (doc, score) in enumerate(hits, start=1):\n",
    "        print(f\"Hit {i}: Score={score:.4f} | Content={doc.page_content[:30]}...\")\n",
    "        snippet_short = (doc.page_content or \"\").strip()\n",
    "        citations.append({\n",
    "            \"citation_id\": f\"{file_id}-c{i}\",\n",
    "            \"snippet\": snippet_short,\n",
    "            \"score\": float(score),\n",
    "            \"rank\": i\n",
    "        })\n",
    "        ctx_snippets.append(f\"[{i}] {snippet_short}\")\n",
    "        scores.append(float(score))\n",
    "        \n",
    "    context_text = \"\\n\\n\".join(ctx_snippets) if ctx_snippets else \"\"\n",
    "    ok_by_score = _score_ok(scores)\n",
    "    \n",
    "    ok_by_llm = True\n",
    "    if not ok_by_score:\n",
    "        print(\"Invoking Grader LLM...\")\n",
    "        grader = _get_grader()\n",
    "        grade_prompt = GRADE_PROMPT.format(context=context_text, question=question)\n",
    "        decision = await grader.ainvoke([{\"role\": \"user\", \"content\": grade_prompt}])\n",
    "        content = decision.content or \"\"\n",
    "        print(f\"Grader Output: {content}\")\n",
    "        ok_by_llm = \"yes\" in content.lower()\n",
    "    \n",
    "    branch = \"with_context\" if ok_by_llm else \"no_context\"\n",
    "    return citations, context_text if branch == \"with_context\" else \"\"\n",
    "\n",
    "# 测试\n",
    "try:\n",
    "    print(\">>> Test Case 1: Relevant Question\")\n",
    "    res1 = await retrieve(\"RAG 系统里的 Generation 是什么意思？\", DEBUG_FILE_ID)\n",
    "    print(f\"Result Branch: {'Found Context' if res1[1] else 'No Context'}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in Test Case 1: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be177b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: 调试流式回答 (answer_stream)\n",
    "async def answer_stream(\n",
    "    question: str,\n",
    "    citations: list[dict],\n",
    "    context_text: str,\n",
    "    branch: str,\n",
    "    session_id: str | None = None\n",
    ") -> AsyncGenerator[dict, None]:\n",
    "    if branch == \"with_context\" and citations:\n",
    "        for c in citations:\n",
    "            yield {\"type\": \"citation\", \"data\": c}\n",
    "    \n",
    "    llm = _get_llm()\n",
    "    history_msgs = get_history(session_id) if session_id else []\n",
    "\n",
    "    if branch == \"with_context\" and context_text:\n",
    "        user_prompt = ANSWER_WITH_CONTEXT.format(question=question, context=context_text)\n",
    "    else:\n",
    "        user_prompt = ANSWER_NO_CONTEXT.format(question=question)\n",
    "\n",
    "    msgs = [{\"role\": \"system\", \"content\": SYSTEM_INSTRUCTION}]\n",
    "    msgs.extend(history_msgs)\n",
    "    msgs.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    final_text_parts = []\n",
    "    try:\n",
    "        async for chunk in llm.astream(msgs):\n",
    "            delta = getattr(chunk, \"content\", None)\n",
    "            if delta:\n",
    "                final_text_parts.append(delta)\n",
    "                yield {\"type\": \"token\", \"data\": delta}\n",
    "    except Exception as e:\n",
    "        yield {\"type\": \"token\", \"data\": f\"Error: {e}\"}\n",
    "\n",
    "    if session_id:\n",
    "        append_history(session_id, \"user\", question)\n",
    "        append_history(session_id, \"assistant\", \"\".join(final_text_parts))\n",
    "\n",
    "    yield {\"type\": \"done\", \"data\": {\"used_retrieval\": branch == \"with_context\"}}\n",
    "\n",
    "async def test_pipeline(question):\n",
    "    print(f\"\\n====== User Question: {question} ======\")\n",
    "    citations, ctx_text = await retrieve(question, DEBUG_FILE_ID)\n",
    "    branch = \"with_context\" if ctx_text else \"no_context\"\n",
    "    print(\"------ Streaming Response ------\")\n",
    "    async for event in answer_stream(question, citations, ctx_text, branch, \"test_sess\"):\n",
    "        if event[\"type\"] == \"token\":\n",
    "            print(event[\"data\"], end=\"\", flush=True)\n",
    "        elif event[\"type\"] == \"done\":\n",
    "            print(f\"\\n[Done]\")\n",
    "\n",
    "try:\n",
    "    await test_pipeline(\"RAG 中的 Retrieval 是什么？\")\n",
    "except Exception as e:\n",
    "    print(f\"Total Pipeline Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
